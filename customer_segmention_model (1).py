# -*- coding: utf-8 -*-
"""customer_segmention_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11tDdwgRgJ4sQl7emZgZuRAWKRYipvbuy
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

df=pd.read_excel('/content/marketing_campaign1 (1) (1).xlsx')
df

df.info()

df.describe()

df.tail()

df.shape

df.iloc[0:1]

df.loc[:9]

df.head()

df.corr(numeric_only=True)

df.dtypes

#handling missing value
df.isnull().sum()

df.isna().sum()

df.duplicated().sum()

df.nunique()

df.iloc[0]

df.iloc[:4]

#selecting rows based on cnditionals
df[df['Income']>60000]

df[df['Income']>60000].head()

df[df['Income']>60000].tail()

df[df['Income']>60000].shape

df[df['Year_Birth']>1990]

df[df['Marital_Status'] == 'Married']

#sorting value
df.sort_values(by='Income')

df.sort_values(by='Income', ascending=False)

df.sort_values(by='Income', ascending=False).head()

df.sort_values(by='Marital_Status').head()

df.sort_values(by='Marital_Status', ascending=False).head()

#finding minium max sum average and count
df['Income'].min()

df['Income'].max()

df['Income'].mean()

df['Income'].count()

df['Income'].sum()

df['Income'].median()

df['Income'].std()

df['Income'].var()

df['Income'].mode()

df['Income'].skew()

df['Income'].kurt()

df['Income'].describe()



df['Income'].value_counts()

#finding unique value
df['Income'].unique()

df['Income'].nunique()

df['Marital_Status'].unique()

df['Marital_Status'].nunique()

df['NumWebVisitsMonth'].nunique()

df['NumWebVisitsMonth'].unique()

df['Year_Birth'].unique()

df['Year_Birth'].nunique()

df['Education'].unique()

df['Education'].nunique()

df['Kidhome'].unique()

#groupby
df.groupby('Income').mean(numeric_only=True)

df.groupby('Income').max(numeric_only=True)

df.groupby('Income').min(numeric_only=True)

df.groupby('Income').sum(numeric_only=True)

df.groupby('Income').count()

#grouping rows by time
df.groupby('Year_Birth').mean(numeric_only=True)

df['Year_Birth']=np.where(df['Year_Birth']<1990,1990,df['Year_Birth'])
df['Year_Birth']=np.where(df['Year_Birth']<1980,1980,df['Year_Birth'])
df

df['Marital_Status']=np.random.choice(['Married','Single','Divorced'],size=len(df))
df

df['Education']=np.random.choice(['Graduation','PhD','Master','Basic','2n Cycle'],size=len(df))
df

df['NumWebVisitsMonth']=np.random.randint(1,10,size=len(df))
df

#Aggregating operation and statistics
df.agg({'Income':['mean','max','min','sum','count']})

df.agg({'Year_Birth':['mean','max','min','sum','count']})

df.agg({'Kidhome':['mean','max','min','sum','count']})

df.agg({'NumWebVisitsMonth':['mean','max','min','sum','count']})

df.agg({'Teenhome':['mean','max','min','sum','count']})

df.agg({'Recency':['mean','max','min','sum','count']})

df.agg({'MntWines':['mean','max','min','sum','count']})

df.groupby('Marital_Status').agg({'Income':['mean','max','min','sum','count']})

df.groupby('Education').agg({'Income':['mean','max','min','sum','count']})

df.groupby('NumWebVisitsMonth').agg({'Income':['mean','max','min','sum','count']})

df.groupby('Teenhome').agg({'Income':['mean','max','min','sum','count']})

df.groupby('Recency').agg({'Income':['mean','max','min','sum','count']})

df.groupby('MntWines').agg({'Income':['mean','max','min','sum','count']})

df.groupby('Marital_Status').agg({'Year_Birth':['mean','max','min','sum','count']})

df.groupby('Education').agg({'Year_Birth':['mean','max','min','sum','count']})

df.groupby('NumWebVisitsMonth').agg({'Year_Birth':['mean','max','min','sum','count']})

df.groupby('Teenhome').agg({'Year_Birth':['mean','max','min','sum','count']})

df.groupby('Recency').agg({'Year_Birth':['mean','max','min','sum','count']})

df.groupby('MntWines').agg({'Year_Birth':['mean','max','min','sum','count']})

df.groupby('Marital_Status').agg({'Kidhome':['mean','max','min','sum','count']})

df.groupby('Education').apply(lambda x:x.count())

df.groupby('NumWebVisitsMonth').apply(lambda x:x.count())

df.groupby('Teenhome').apply(lambda x:x.count())

df.groupby('Recency').apply(lambda x:x.count())

df.groupby('MntWines').apply(lambda x:x.count())

df.groupby('Marital_Status').apply(lambda x:x.count())

# Visualize distributions of key features
sns.histplot(df['Income'], bins=20, kde=True)
plt.title('Distribution of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['Year_Birth'], bins=20, kde=True)
plt.title('Distribution of Year of Birth')
plt.xlabel('Year of Birth')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['Kidhome'], bins=20, kde=True)
plt.title('Distribution of Kidhome')
plt.xlabel('Kidhome')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['Teenhome'], bins=20, kde=True)
plt.title('Distribution of Teenhome')
plt.xlabel('Teenhome')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['Recency'], bins=20, kde=True)
plt.title('Distribution of Recency')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['MntWines'], bins=20, kde=True)
plt.title('Distribution of MntWines')
plt.xlabel('MntWines')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['NumWebVisitsMonth'], bins=20, kde=True)
plt.title('Distribution of NumWebVisitsMonth')
plt.xlabel('NumWebVisitsMonth')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['MntFruits'], bins=20, kde=True)
plt.title('Distribution of MntFruits')
plt.xlabel('MntFruits')
plt.ylabel('Frequency')
plt.show()

sns.histplot(df['Marital_Status'], bins=20, kde=True)
plt.title('Distribution of Marital_Status')
plt.xlabel('Marital_Status')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='Income', data=df, color='skyblue')
plt.title('Boxen Plot of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='Year_Birth', data=df, color='skyblue')
plt.title('Boxen Plot of Year_Birth')
plt.xlabel('Year_Birth')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='Kidhome', data=df, color='skyblue')
plt.title('Boxen Plot of Kidhome')
plt.xlabel('Kidhome')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='Teenhome', data=df, color='skyblue')
plt.title('Boxen Plot of Teenhome')
plt.xlabel('Teenhome')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='Recency', data=df, color='skyblue')
plt.title('Boxen Plot of Recency')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='MntWines', data=df, color='skyblue')
plt.title('Boxen Plot of MntWines')
plt.xlabel('MntWines')
plt.ylabel('Frequency')
plt.show()

sns.boxenplot(x='NumWebVisitsMonth', data=df, color='skyblue')
plt.title('Boxen Plot of NumWebVisitsMonth')
plt.xlabel('NumWebVisitsMonth')
plt.ylabel('Frequency')
plt.show()

#density plot
sns.kdeplot(data=df, x='Income', fill=True, color='skyblue', alpha=0.5, linewidth=2, bw_adjust=1, label='Income')
plt.title('Kernel Density Plot of Income')
plt.xlabel('Income')
plt.ylabel('Density')
plt.show()

sns.kdeplot(data=df, x='Year_Birth', fill=True, color='skyblue', alpha=0.5, linewidth=2, bw_adjust=1, label='Year_Birth')
plt.title('Kernel Density Plot of Year_Birth')
plt.xlabel('Year_Birth')
plt.ylabel('Density')
plt.show()

sns.kdeplot(data=df, x='Kidhome', fill=True, color='skyblue', alpha=0, linewidth=2, bw_adjust=1, label='Kidhome')
plt.title('Kernel Density Plot of Kidhome')
plt.xlabel('Kidhome')
plt.ylabel('Density')
plt.show()

sns.kdeplot(data=df, x='Teenhome', fill=True, color='skyblue', alpha=0, linewidth=2, bw_adjust=1, label='Teenhome')
plt.title('Kernel Density Plot of Teenhome')
plt.xlabel('Teenhome')
plt.ylabel('Density')
plt.show()

sns.kdeplot(data=df, x='Recency', fill=True, color='skyblue', alpha=0, linewidth=2,bw_adjust=1, label='Recency')
plt.title('Kernel Density Plot of Recency')
plt.xlabel('Recency')
plt.ylabel('Density')
plt.show()

sns.kdeplot(data=df, x='MntWines', fill=True, color='skyblue', alpha=0, linewidth=2, bw_adjust=1, label='MntWines')
plt.title('Kernel Density Plot of MntWines')
plt.xlabel('MntWines')
plt.ylabel('Density')
plt.show()

#violinplot
sns.violinplot(x='Income', data=df, color='skyblue')
plt.title('Violin Plot of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

sns.violinplot(x='Year_Birth', data=df, color='skyblue')
plt.title('Violin Plot of Year_Birth')
plt.xlabel('Year_Birth')
plt.ylabel('Frequency')
plt.show()

sns.barplot(x='Kidhome', data=df, color='skyblue', label='Kidhome')
plt.title('Bar Plot of Kidhome')
plt.xlabel('Kidhome')
plt.ylabel('Frequency')
plt.show()

sns.barplot(x='Teenhome', data=df, color='skyblue', label='Teenhome')
plt.title('Bar Plot of Teenhome')
plt.xlabel('Teenhome')
plt.ylabel('Frequency')
plt.show()

sns.barplot(x='Recency', data=df, color='skyblue', label='Recency')
plt.title('Bar Plot of Recency')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.show()

sns.barplot(x='MntWines', data=df, color='skyblue', label='MntWines')
plt.title('Bar Plot of MntWines')
plt.xlabel('MntWines')
plt.ylabel('Frequency')
plt.show()

sns.barplot(x='NumWebVisitsMonth', data=df, color='skyblue', label='NumWebVisitsMonth')
plt.title('Bar Plot of NumWebVisitsMonth')
plt.xlabel('NumWebVisitsMonth')
plt.ylabel('Frequency')
plt.show()

#pairplot
sns.pairplot(df, hue='Income', palette='coolwarm', height=2.5)
plt.show()

#Hexagonal binning and contours
sns.jointplot(x='Income', y='Year_Birth', data=df, kind='hex', color='skyblue')
plt.title('Hexagonal Binning and Contour Plot')
plt.xlabel('Income')
plt.ylabel('Year_Birth')
plt.show()

sns.jointplot(x='Kidhome', y='Teenhome', data=df, kind='hex', color='skyblue')
plt.title('Hexagonal Binning and Contour Plot')
plt.xlabel('Kidhome')
plt.ylabel('Teenhome')
plt.show()

sns.jointplot(x='Recency', y='MntWines', data=df, kind='hex', color='skyblue')
plt.title('Hexagonal Binning and Contour Plot')
plt.xlabel('Recency')
plt.ylabel('MntWines')

sns.jointplot(x='NumWebVisitsMonth', y='MntFruits', data=df, kind='hex', color='skyblue')
plt.title('Hexagonal Binning and Contour Plot')
plt.xlabel('NumWebVisitsMonth')
plt.ylabel('MntFruits')
plt.show()

sns.jointplot(x='NumWebVisitsMonth', y='MntFruits', data=df, kind='hex', color='skyblue')
plt.title('Hexagonal Binning and Contour Plot')
plt.xlabel('NumWebVisitsMonth')
plt.ylabel('MntFruits')

#contigency table
pd.crosstab(df['Income'], df['Year_Birth'])

#heatmap
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f') # Changed etfs to df
plt.show()

#standard scaler
from sklearn.preprocessing import StandardScaler

# 1. Select numerical features for scaling
numerical_features = df.select_dtypes(include=['number']).columns
print(numerical_features)

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[numerical_features])
df_scaled = pd.DataFrame(df_scaled, columns=numerical_features, index=df.index)
print(df_scaled)

df_scaled = pd.DataFrame(df_scaled, columns=numerical_features, index=df.index)
df_final = pd.concat([df_scaled, df.drop(columns=numerical_features)], axis=1)

print(df_final)

#modelbuilding
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

#split your data
X = df_final.drop('Income', axis=1)
y = df_final['Income']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train

# Create a Preprocessing Pipeline
from sklearn.pipeline import Pipeline # Added import here
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns
categorical_features

# 2. Create transformers for numerical and categorical features
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 3. Create a ColumnTransformer to apply transformers to respective columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# 4. Create a pipeline with preprocessing and the Logistic Regression model
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))  # Increased max_iter
])

# 5. Split your data
X=df_final.drop('Income',axis=1)
y=df_final['Income']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train

#create dummy variables
df = pd.get_dummies(df, columns=['Income'])
df

from sklearn.dummy import DummyRegressor # Import DummyRegressor

y_train = y_train.fillna(y_train.mean())
y_test = y_test.fillna(y_test.mean())
dummy_regressor = DummyRegressor(strategy='mean')
dummy_regressor.fit(X_train, y_train)
y_pred = dummy_regressor.predict(X_test)
print(y_pred)

#get the r squre
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print(r2)

# Assuming 'categorical_features' is the list of categorical columns
# from your previous pre-processing steps

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse=False for KMeans
encoded_data = encoder.fit_transform(X_train[categorical_features])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features), index=X_train.index)

X_train_encoded = X_train.drop(columns=categorical_features).join(encoded_df)
X_train_encoded

# Determine the optimal number of clusters using the Elbow Method
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Assuming 'categorical_features' is the list of categorical columns
# from your previous pre-processing steps

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse=False for KMeans
encoded_data = encoder.fit_transform(X_train[categorical_features])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features), index=X_train.index)

# Convert 'Dt_Customer' to datetime before dropping categorical features
X_train['Dt_Customer'] = pd.to_datetime(X_train['Dt_Customer'])

X_train_encoded = X_train.drop(columns=categorical_features).join(encoded_df)

# Now you can create the numeric representation:
X_train_encoded['Dt_Customer_Numeric'] = (X_train_encoded['Dt_Customer'] - pd.Timestamp('1970-01-01')).dt.days
X_train_encoded = X_train_encoded.drop(columns=['Dt_Customer'])
X_train_encoded

wcss=[]
for i in range(1,11):
    kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42)
    kmeans.fit(X_train_encoded)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Fit K-Means with the optimal number of clusters (e.g., 4)
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans.fit(X_train_encoded)
kmeans.labels_
kmeans.cluster_centers_

# Display cluster centers
print(kmeans.cluster_centers_)

#visulize the cluster
# Instead of using 'Income', you should use the original DataFrame (df) or the y_train for Income values
plt.scatter(X_train_encoded['Dt_Customer_Numeric'], y_train, c=kmeans.labels_, cmap='rainbow')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=300, c='black')
plt.xlabel('Dt_Customer_Numeric')
plt.ylabel('Income')
plt.title('K-Means Clustering')
plt.show()

# Evaluate clustering
silhouette_avg = silhouette_score(X_train_encoded, kmeans.labels_)
print(f"Silhouette Score: {silhouette_avg}")

#create custom metric
from sklearn.metrics import make_scorer
def custom_metric(y_true, y_pred):
  r2 = r2_score(y_true, y_pred)
  return r2

#make scorer and define that higher scores are better
custom_scorer = make_scorer(custom_metric, greater_is_better=True, needs_proba=False, needs_threshold=False)
custom_scorer

#create ridge regression object
from sklearn.linear_model import Ridge
ridge_reg = Ridge()
ridge_reg

model=ridge_reg.fit(X_train_encoded,y_train)
print(model.score(X_train_encoded,y_train))

#apply custome scorer
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
cross_val_score(ridge_reg,X_train_encoded,y_train,cv=5,scoring=custom_scorer)
print(cross_val_score(ridge_reg,X_train_encoded,y_train,cv=5,scoring=custom_scorer))

#visualzing the effect of training set size
from sklearn.model_selection import learning_curve
train_sizes, train_scores, val_scores = learning_curve(ridge_reg, X_train_encoded, y_train)
train_sizes

#create cv training and test scores for various training set sizes
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)
print(train_mean)
print(train_std)
print(val_mean)
print(val_std)

#create means and standard deviation of test set scores
plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, val_mean, label='Validation score')
plt.plot(train_sizes, train_mean + train_std, color='blue', alpha=0.1)
plt.plot(train_sizes, train_mean - train_std, color='blue', alpha=0.1)
plt.xlabel('Training Set Size')
plt.ylabel('R-squared Score')
plt.title('Learning Curve')
plt.legend()
plt.show()

#pca
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
X_train_pca=pca.fit_transform(X_train_encoded)
X_train_pca

#evlolate
pca.explained_variance_ratio_

#pca plot
plt.scatter(X_train_pca[:,0],X_train_pca[:,1],c=kmeans.labels_,cmap='rainbow')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA Plot')
plt.show()

!pip install streamlit

import streamlit as st
st.title('Marketing campaign analysis')

st.dataframe(df)

# Create a scatter plot
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=kmeans.labels_, cmap='rainbow')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA Plot')

# Display the plot in Streamlit
st.pyplot